---
title: "Is there a Difference if we ignore Profession?"
author: "Aaron"
date: "02/09/2018"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_float: yes
---
  
```{r setup, include=FALSE}
library(here)
library(tidyverse)
library(recipes)
library(rsample)
library(rpartScore)
library(pander)
library(rpart.plot)
library(tidyposterior)
library(DescTools)
library(skimr)

source(here("scripts", "00functions.R"))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(cache.lazy = FALSE)
knitr::read_chunk(here("scripts", "01load.R"))
knitr::read_chunk(here("scripts", "02preprocess.R"))
knitr::read_chunk(here("scripts", "03rpart.R"))
knitr::read_chunk(here("scripts", "04measures.R"))
knitr::read_chunk(here("scripts", "05posterior.R"))
skim_with(numeric = list(p25 = NULL, p75 = NULL, missing = NULL))
```

# Preparing

Everthing gets standatized, inside the resamples.

```{r load}
```

# Descriptives

```{r predictors}
```

```{r}
data %>% select(one_of(predictors_i))  %>% skim()
```


# Fit Model & Cross Validate

We repeat a ten fold crossvalidation ten times. The model is a classification tree for ordinal response. The tree is grown using Generalized Gini impurity function, with absolut difference in scores. The pruning is based on the total misclassification rate. These settings were found to perform best in a hyperparamter gird search and the same cross-validation procedure.

```{r resample}
```

```{r recipe}
```

```{r cache}
refit_rpart <- !fs::file_exists(here("out", "data_cv.rds"))
```

```{r rpart, eval = refit_rpart}
```

```{r cache2, eval = !refit_rpart}
data_cv <- read_rds(here("out", "data_cv.rds"))
```

```{r prediction}
```

```{r accuracy}
```

```{r cramerv}
```

# Refit Model to Full Datset

There where two different models one with the variable Berufgruppe (indicated via suffix `_i` or "include") or without (indicated with suffix `_e` or excluded).

```{r}
fit_rpart <- function(recipe, ...){
  #browser()
  pred <- recipe %>% juice(all_predictors()) %>% names()
  out <- recipe %>% juice(all_outcomes()) %>% names()
  model_formula <- as.formula(paste0(out, " ~ ", paste(pred, collapse = " + ")))
  rpartScore(model_formula,
        data = juice(recipe, everything(), composition = "data.frame"),
        ...)
}
```

## Included

```{r, cache=TRUE}
recipe_i %>%
  prep(data, retain = TRUE) %>%
  fit_rpart(model = TRUE,
            split = "abs",
            prune = "mr") %>% 
  rpart.plot(fallen.leaves = F)
```

## Excluded

```{r, cache=TRUE}
recipe_e %>%
  prep(data, retain = TRUE) %>%
  fit_rpart(model = TRUE,
            split = "abs",
            prune = "mr") %>% 
  rpart.plot(fallen.leaves = F)
```

# Compare included & excluded

## Descriptive

```{r}
data_cv %>%
  select(acc_i, acc_e, cramerv_i, cramerv_e) %>% 
  gather() %>% 
  mutate(measure = str_remove(model, "_[i,e]$"),
         model = if_else(str_extract(model, "[i,e]$") == "i", "included", "excluded")) %>% 
  ggplot(aes(statistic, color = model)) +
  geom_density() +
  facet_wrap(~measure) +
  theme_minimal() +
  NULL

data_cv %>%
  select(acc_i, acc_e, cramerv_i, cramerv_e)
```

### Accuracy

```{r acc-stacked}
```

```{r}
acc_stacked %>%
  unite(id, id, id2) %>% 
  ggplot(aes(x = model, y = statistic, group = id, col = id)) + 
  geom_line(alpha = .75) + 
  theme_minimal() +
  theme(legend.position = "none") +
  ylab("accuracy") +
  NULL
```

```{r cramerv-stacked}
```

```{r}
cramerv_stacked %>% 
  unite(id, id, id2) %>% 
  ggplot(aes(x = model, y = statistic, group = id, col = id)) + 
  geom_line(alpha = .75) + 
  theme_minimal() +
  theme(legend.position = "none") +
  ylab("Craner's V") +
  NULL
```

## Posterior Distribution for Difference in Accuracy

```{r acc-model, results='hide'}
```

```{r}
acc_model$stan
summary(acc_vs) %>%
  select(contrast, probability, mean, lower, upper) %>%
  pander()
ggplot(acc_vs) + theme_minimal()
```

## Posterior Distribution for Difference in Cramer's V

```{r cramerv-model, results='hide'}
```

```{r}
cramerv_model$stan
summary(cramerv_vs) %>%
  select(contrast, probability, mean, lower, upper) %>%
  pander()
ggplot(acc_vs) + theme_minimal()
```
