---
title: "Methods"
author: "Aaron"
date: "9/4/2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
library(here)
library(tidyverse)
library(recipes)
library(rsample)
library(rpartScore)
library(pander)
library(rpart.plot)
library(tidyposterior)
library(DescTools)
library(skimr)

source(here("scripts", "00functions.R"))
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(cache.lazy = FALSE)
knitr::read_chunk(here("scripts", "01load.R"))
knitr::read_chunk(here("scripts", "02preprocess.R"))
knitr::read_chunk(here("scripts", "03rpart.R"))
knitr::read_chunk(here("scripts", "04measures.R"))
knitr::read_chunk(here("scripts", "05posterior.R"))
skim_with(numeric = list(p25 = NULL, p75 = NULL, missing = NULL))
```

## Predictors

```{r load}
```

```{r predictors}
```

```{r resample}
```

```{r recipe}
```

```{r}
pander(model_formula_i)
```

## Model Building

The model is a classification tree for ordinal response. The tree is grown using Generalized Gini impurity function, with absolut difference in scores. The pruning is based on the total misclassification rate. These settings were found to perform best in a hyperparamter gridsearch with the cross-validation procedure described below.

## Generalized Gini Impurity

A classical classification tree splits gredy so that the endnodes are pure (one class is dominating the node). For example in this table prurity would favour solution A (purity of A = 11/(11+2+8) = 52,38%; B = 10/(10+8+3) = 47,62%:

| class | A  | B  |
|-------|----|----|
| 1     | 11 | 10 |
| 2     | 2  | 8  |
| 3     | 8  | 3  |

## Generalized Gini Impurity

However, taking into account the ordinal structure, that might not lead to optimal results. Instead the ordinal structure makes it possible to calculate a distance between possible cuts and maximize this distance. Such a distance (with other favorible properties, such as monotone exclusivity preference property) is the Generalized Gini Impurity. Aditionally one might sqaure the optained score to punish more for extreme points. This option was explored but did not perform better then the absolute distance in the crossfolding.

## Pruning

The best tree size (through pruning) is, due to the software implementation in the moment, found via another 10-fold cross folding, within our own crossvolding procedure. This is unnassecary and could be evaluated as another hyperparamter directly in the first level gridsearch, but shouldn't make a big difference.

However the godness of pruning can be found via minimizing the risk. Risk can either be defined as misclassifaction cost or misclassification rate. In the crossvalidation misclassification rate performed better. 

## References

Breiman L., Friedman J.H., Olshen R.A., Stone C.J. 1984 Classification and Regression Trees. Wadsworth International.

Galimberti G., Soffritti G., Di Maso M. 2012 Classification Trees for Ordinal Responses in R: The rpartScore Package. Journal of Statistical Software, 47(10), 1-25. URL http://www.jstatsoft.org/v47/i10/.

Piccarreta R. 2008 Classication Trees for Ordinal Variables. Computational Statistics, 23, 407-427.

## Model Performance Evaluation

The models were evaluated with Cramers V and plain accuracy.

## References

Agresti, Alan (1996) Introduction to categorical data analysis. NY: John Wiley and Sons

Sakoda, J.M. (1977) Measures of Association for Multivariate Contingency Tables, Proceedings of the Social Statistics Section of the American Statistical Association (Part III), 777-780.

Yule, G. Uday (1912) On the methods of measuring association between two attributes. Journal of the Royal Statistical Society, LXXV, 579-652

## Model Performance Estimates

The estimates for the evaluation measures were found using ten times repated ten fold cross validation. Therefor all estimates given here are on unseen data.



## Model Performance Estimates

These estimates set expactation how a model with these setting would perform on unseen data. From this follow two things:

1. we only evaluate the generalizing capacity of the model, meaning a model that only represents the training set will be evaluated badly

2. its not a direct estimate for the final model, but instead the expactation one has for a model which is build the way the final model is build for an equivalent data generating process

```{r cache}
refit_rpart <- !fs::file_exists(here("out", "data_cv.rds"))
```

```{r rpart, eval = refit_rpart}
```

```{r cache2, eval = !refit_rpart}
data_cv <- read_rds(here("out", "data_cv.rds"))
```

```{r prediction}
```

```{r accuracy}
```

```{r cramerv}
```

## Final Model

```{r message=FALSE, warning=FALSE, cache=TRUE}
fit_rpart <- function(recipe, ...){
  #browser()
  pred <- recipe %>% juice(all_predictors()) %>% names()
  out <- recipe %>% juice(all_outcomes()) %>% names()
  model_formula <- as.formula(paste0(out, " ~ ", paste(pred, collapse = " + ")))
  rpartScore(model_formula,
        data = juice(recipe, everything(), composition = "data.frame"),
        ...)
}
recipe_i %>%
  prep(data, retain = TRUE) %>%
  fit_rpart(model = TRUE,
            split = "abs",
            prune = "mr") %>% 
  rpart.plot(fallen.leaves = F)
```

# Difference

## Difference

To test the hypothesis that the profession is an important predictor (that it contributes uniquely to the prediction power) a bayssian mixed effect models is utilzed. To perform such a model we have estimated the predictive power of the same model as described above without the predictor profession.

From this process we obtained a datset, where the two models are fited on the same test/holdout samples. Therefore we have 100 (10 folds, repated ten times) data points on the performance of these to models.

## Model without profession

```{r message=FALSE, warning=FALSE, cache=TRUE}
recipe_e %>%
  prep(data, retain = TRUE) %>%
  fit_rpart(model = TRUE,
            split = "abs",
            prune = "mr") %>% 
  rpart.plot(fallen.leaves = F)
```

## Descriptive

Descriptive there is a difference.

```{r}
density_plot <- data_cv %>%
  select(acc_i, acc_e, cramerv_i, cramerv_e) %>% 
  gather() %>% 
  mutate(measure = str_remove(model, "_[i,e]$"),
         model = if_else(str_extract(model, "[i,e]$") == "i", "included", "excluded"))
max_dens <- function(x){
  dens <- density(x)
  out <- dens$x[which.max(dens$y)]
  return(out)
}
density_plot_stats <- density_plot %>%
  group_by(model, measure) %>% 
  summarise(mean = mean(statistic),
            `Max. density` = max_dens(statistic)) %>% 
  gather("statistic", "value", mean, `Max. density`)
density_plot %>% 
  ggplot(aes(statistic, color = model)) +
  geom_density() +
  geom_vline(data = density_plot_stats, aes(xintercept = value, color = model, linetype = statistic)) +
  facet_wrap(~measure) +
  theme_minimal() +
  NULL
```

## Descriptive

```{r}
data_cv %>%
  select(acc_i, acc_e, cramerv_i, cramerv_e) %>% 
  skim()
```


## Sampling ACC

But some of the variance is due to the sampling.

```{r acc-stacked}
```

```{r}
acc_stacked %>%
  unite(id, id, id2) %>% 
  ggplot(aes(x = model, y = statistic, group = id, col = id)) + 
  geom_line(alpha = .75) + 
  theme_minimal() +
  theme(legend.position = "none") +
  ylab("accuracy") +
  NULL
```

## Sampling CramerV

```{r cramerv-stacked}
```

```{r}
cramerv_stacked %>% 
  unite(id, id, id2) %>% 
  ggplot(aes(x = model, y = statistic, group = id, col = id)) + 
  geom_line(alpha = .75) + 
  theme_minimal() +
  theme(legend.position = "none") +
  ylab("Craner's V") +
  NULL
```

## Significant?

Using the evaluation measures (CramerV and ACC) as dependent variables, the folds as randome effect and the inclusion of professen as a fixed effect:

```{r acc-model, results='hide'}
```

```{r cramerv-model, results='hide'}
```

```{r}
rbind(summary(acc_vs), summary(cramerv_vs)) %>%
  select(contrast, probability, mean, lower, upper) %>%
  pander()
```

## ACC

```{r}
ggplot(acc_vs) + theme_minimal()
```

## CramerV

```{r}
ggplot(cramerv_vs) + theme_minimal()
```

## Significant?

This procedure is losely absed/adapted from:

Benavoli, A., Corani, G., Dem≈°ar, J., & Zaffalon, M. (2017). Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis. The Journal of Machine Learning Research, 18(1), 2653-2688.
